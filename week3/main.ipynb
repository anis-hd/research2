{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU configuration error: Virtual devices cannot be modified after being initialized\n",
      "\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_36/kernel:0', 'conv2d_36/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['conv2d_36/kernel:0', 'conv2d_36/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 124\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_gen)):\n\u001b[0;32m    123\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m train_gen[i]\n\u001b[1;32m--> 124\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;66;03m# Clear memory\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py:2381\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2377\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39msingle_batch_iterator(\n\u001b[0;32m   2378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[0;32m   2379\u001b[0m     )\n\u001b[0;32m   2380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[1;32m-> 2381\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2383\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:980\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[0;32m    977\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    978\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;66;03m# stateless function.\u001b[39;00m\n\u001b[1;32m--> 980\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    982\u001b[0m   _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    983\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn\u001b[38;5;241m.\u001b[39m_function_spec\u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    984\u001b[0m           \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "num_frames = 50\n",
    "target_height = 140\n",
    "batch_size = 1  # Minimum batch size for maximum memory efficiency\n",
    "\n",
    "def extract_frames(video_path, num_frames):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while len(frames) < num_frames and cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def create_video(frames, output_path, fps, resolution):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    height, width = resolution\n",
    "    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    for frame in frames:\n",
    "        writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "    writer.release()\n",
    "\n",
    "def build_minimal_sr_model(lr_shape, hr_shape):\n",
    "    inputs = Input(shape=lr_shape)\n",
    "    \n",
    "    # Feature extraction\n",
    "    x = Conv2D(8, 3, padding='same', activation='relu')(inputs)\n",
    "    \n",
    "    # Upscaling to target resolution\n",
    "    x = Lambda(lambda x: tf.image.resize(x, hr_shape[:2], \n",
    "               method='bicubic'))(x)\n",
    "    \n",
    "    # Final convolution\n",
    "    outputs = Conv2D(3, 3, padding='same', activation='sigmoid')(x)\n",
    "    \n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# ----------------- GPU Configuration -----------------\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Configure GPU with memory limits\n",
    "        tf.config.set_logical_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.LogicalDeviceConfiguration(memory_limit=2048)]  # 2GB limit\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "\n",
    "# ----------------- Data Pipeline -----------------\n",
    "class FrameGenerator:\n",
    "    def __init__(self, lr_frames, hr_frames):\n",
    "        self.lr_frames = lr_frames\n",
    "        self.hr_frames = hr_frames\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.lr_frames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        lr = self.lr_frames[idx] / 255.0\n",
    "        hr = self.hr_frames[idx] / 255.0\n",
    "        return np.expand_dims(lr, axis=0), np.expand_dims(hr, axis=0)\n",
    "\n",
    "# ----------------- Processing -----------------\n",
    "# Extract frames\n",
    "hr_frames = extract_frames('input.mp4', num_frames)\n",
    "if not hr_frames:\n",
    "    raise ValueError(\"No frames extracted from input video\")\n",
    "\n",
    "# Get video properties\n",
    "original_height, original_width = hr_frames[0].shape[:2]\n",
    "cap = cv2.VideoCapture('input.mp4')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "cap.release()\n",
    "\n",
    "# Create low-res frames\n",
    "lr_frames = []\n",
    "for frame in hr_frames:\n",
    "    aspect_ratio = original_width / original_height\n",
    "    new_width = int(target_height * aspect_ratio)\n",
    "    lr_frame = cv2.resize(frame, (new_width, target_height), interpolation=cv2.INTER_AREA)\n",
    "    lr_frames.append(lr_frame)\n",
    "\n",
    "# Create compressed video\n",
    "create_video(lr_frames, 'compressed.mp4', fps, (target_height, new_width))\n",
    "\n",
    "# Prepare data generator\n",
    "train_gen = FrameGenerator(lr_frames, hr_frames)\n",
    "\n",
    "# Clear CPU memory\n",
    "del hr_frames\n",
    "gc.collect()\n",
    "\n",
    "# ----------------- Training Setup -----------------\n",
    "try:\n",
    "    # Build model with correct output dimensions\n",
    "    model = build_minimal_sr_model(lr_frames[0].shape, (original_height, original_width, 3))\n",
    "    \n",
    "    # Compile with reduced precision\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-4),\n",
    "        loss='mae'\n",
    "    )\n",
    "    \n",
    "    # Training loop with progress monitoring\n",
    "    for epoch in range(20):  # Reduced epochs\n",
    "        print(f\"\\nEpoch {epoch+1}/20\")\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i in range(len(train_gen)):\n",
    "            x, y = train_gen[i]\n",
    "            loss = model.train_on_batch(x, y)\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            # Clear memory\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "            \n",
    "            # Print progress\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Processed {i+1}/{len(train_gen)} frames - Loss: {loss:.4f}\")\n",
    "        \n",
    "        # Save model checkpoint\n",
    "        model.save(f'sr_model_epoch_{epoch+1}.h5')\n",
    "        print(f\"Epoch {epoch+1} completed - Average Loss: {epoch_loss/len(train_gen):.4f}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Training failed: {str(e)}\")\n",
    "\n",
    "# ----------------- Inference -----------------\n",
    "predicted = []\n",
    "for i in range(len(lr_frames)):\n",
    "    x = np.expand_dims(lr_frames[i] / 255.0, axis=0)\n",
    "    pred = model.predict(x, verbose=0)\n",
    "    predicted.append((pred[0] * 255).astype(np.uint8))\n",
    "    \n",
    "    # Clear memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Print progress\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Processed {i+1}/{len(lr_frames)} frames for reconstruction\")\n",
    "\n",
    "# Create output video\n",
    "create_video(predicted, 'output.mp4', fps, (original_height, original_width))\n",
    "\n",
    "print(\"Processing complete. Results saved in output.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV bindings requires \"numpy\" package.\n",
      "Install it via command:\n",
      "    pip install numpy\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\cv2\\__init__.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiarray\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Add, Conv2DTranspose, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "num_frames = 50\n",
    "target_height = 140\n",
    "batch_size = 1  # Minimum batch size for maximum memory efficiency\n",
    "\n",
    "# ----------------- Helper Functions -----------------\n",
    "def extract_frames(video_path, num_frames):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while len(frames) < num_frames and cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def create_video(frames, output_path, fps, resolution):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    height, width = resolution\n",
    "    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    for frame in frames:\n",
    "        writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "    writer.release()\n",
    "\n",
    "# ----------------- Improved Model Architecture -----------------\n",
    "def build_improved_sr_model(lr_shape, hr_shape):\n",
    "    inputs = Input(shape=lr_shape)\n",
    "    \n",
    "    # Initial feature extraction\n",
    "    x = Conv2D(64, 3, padding='same', activation='relu')(inputs)\n",
    "    \n",
    "    # Residual blocks\n",
    "    for _ in range(4):\n",
    "        residual = x\n",
    "        x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "        x = Conv2D(64, 3, padding='same')(x)\n",
    "        x = Add()([x, residual])\n",
    "    \n",
    "    # Upsampling\n",
    "    x = Conv2DTranspose(64, 3, strides=2, padding='same', activation='relu')(x)\n",
    "    x = Conv2DTranspose(64, 3, strides=2, padding='same', activation='relu')(x)\n",
    "    \n",
    "    # Final convolution\n",
    "    outputs = Conv2D(3, 3, padding='same', activation='sigmoid')(x)\n",
    "    \n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# ----------------- Perceptual Loss -----------------\n",
    "def perceptual_loss(y_true, y_pred):\n",
    "    vgg = VGG19(include_top=False, weights='imagenet', input_shape=(None, None, 3))\n",
    "    vgg = Model(vgg.input, vgg.layers[20].output)\n",
    "    vgg.trainable = False\n",
    "    \n",
    "    true_features = vgg(y_true)\n",
    "    pred_features = vgg(y_pred)\n",
    "    \n",
    "    return tf.reduce_mean(tf.square(true_features - pred_features))\n",
    "\n",
    "# ----------------- Data Pipeline -----------------\n",
    "class FrameGenerator:\n",
    "    def __init__(self, lr_frames, hr_frames):\n",
    "        self.lr_frames = lr_frames\n",
    "        self.hr_frames = hr_frames\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.lr_frames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        lr = self.lr_frames[idx] / 255.0\n",
    "        hr = self.hr_frames[idx] / 255.0\n",
    "        return np.expand_dims(lr, axis=0), np.expand_dims(hr, axis=0)\n",
    "\n",
    "# ----------------- PSNR and SSIM Calculation -----------------\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    max_pixel = 255.0\n",
    "    return 20 * np.log10(max_pixel / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    # SSIM calculation using OpenCV\n",
    "    return cv2.SSIM(img1, img2)\n",
    "\n",
    "# ----------------- Processing -----------------\n",
    "# Extract frames\n",
    "hr_frames = extract_frames('input.mp4', num_frames)\n",
    "if not hr_frames:\n",
    "    raise ValueError(\"No frames extracted from input video\")\n",
    "\n",
    "# Get video properties\n",
    "original_height, original_width = hr_frames[0].shape[:2]\n",
    "cap = cv2.VideoCapture('input.mp4')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "cap.release()\n",
    "\n",
    "# Create low-res frames\n",
    "lr_frames = []\n",
    "for frame in hr_frames:\n",
    "    aspect_ratio = original_width / original_height\n",
    "    new_width = int(target_height * aspect_ratio)\n",
    "    lr_frame = cv2.resize(frame, (new_width, target_height), interpolation=cv2.INTER_AREA)\n",
    "    lr_frames.append(lr_frame)\n",
    "\n",
    "# Create compressed video\n",
    "create_video(lr_frames, 'compressed.mp4', fps, (target_height, new_width))\n",
    "\n",
    "# Prepare data generator\n",
    "train_gen = FrameGenerator(lr_frames, hr_frames)\n",
    "\n",
    "# Clear CPU memory\n",
    "del hr_frames\n",
    "gc.collect()\n",
    "\n",
    "# ----------------- Training Setup -----------------\n",
    "try:\n",
    "    # Build improved model\n",
    "    model = build_improved_sr_model(lr_frames[0].shape, (original_height, original_width, 3))\n",
    "    \n",
    "    # Compile with perceptual loss and Adam optimizer\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-4),\n",
    "        loss=perceptual_loss\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "    \n",
    "    # Training loop with progress monitoring\n",
    "    for epoch in range(20):  # Reduced epochs\n",
    "        print(f\"\\nEpoch {epoch+1}/20\")\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i in range(len(train_gen)):\n",
    "            x, y = train_gen[i]\n",
    "            loss = model.train_on_batch(x, y)\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            # Clear memory\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "            \n",
    "            # Print progress\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Processed {i+1}/{len(train_gen)} frames - Loss: {loss:.4f}\")\n",
    "        \n",
    "        # Save model checkpoint\n",
    "        model.save(f'sr_model_epoch_{epoch+1}.h5')\n",
    "        print(f\"Epoch {epoch+1} completed - Average Loss: {epoch_loss/len(train_gen):.4f}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Training failed: {str(e)}\")\n",
    "\n",
    "# ----------------- Inference -----------------\n",
    "predicted = []\n",
    "for i in range(len(lr_frames)):\n",
    "    x = np.expand_dims(lr_frames[i] / 255.0, axis=0)\n",
    "    pred = model.predict(x, verbose=0)\n",
    "    predicted.append((pred[0] * 255).astype(np.uint8))\n",
    "    \n",
    "    # Clear memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Print progress\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Processed {i+1}/{len(lr_frames)} frames for reconstruction\")\n",
    "\n",
    "# Create output video\n",
    "create_video(predicted, 'output.mp4', fps, (original_height, original_width))\n",
    "\n",
    "# Evaluate PSNR and SSIM\n",
    "psnr_values = []\n",
    "ssim_values = []\n",
    "for i in range(len(predicted)):\n",
    "    psnr_values.append(calculate_psnr(lr_frames[i], predicted[i]))\n",
    "    ssim_values.append(calculate_ssim(lr_frames[i], predicted[i]))\n",
    "\n",
    "print(f\"Average PSNR: {np.mean(psnr_values):.2f}\")\n",
    "print(f\"Average SSIM: {np.mean(ssim_values):.4f}\")\n",
    "\n",
    "print(\"Processing complete. Results saved in output.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_compression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtfc\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Helper: GDN and Inverse GDN layers from tfc\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\__init__.py:37\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mabsl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msix\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m function_pb2\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_compression as tfc\n",
    "\n",
    "# -------------------------------\n",
    "# Helper: GDN and Inverse GDN layers from tfc\n",
    "# -------------------------------\n",
    "GDN = tfc.layers.GDN\n",
    "IGDN = tfc.layers.GDN(inverse=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Module 1: Motion Estimation Network (Pyramid Network)\n",
    "# -------------------------------\n",
    "class PyramidMotionEstimator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(PyramidMotionEstimator, self).__init__()\n",
    "        # For simplicity, we implement a single-scale network\n",
    "        # In practice, you would build a 5-level pyramid. Here we mimic one level.\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, kernel_size=7, padding='same', activation='relu')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, kernel_size=7, padding='same', activation='relu')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(32, kernel_size=7, padding='same', activation='relu')\n",
    "        self.conv4 = tf.keras.layers.Conv2D(16, kernel_size=7, padding='same', activation='relu')\n",
    "        self.conv5 = tf.keras.layers.Conv2D(2,  kernel_size=7, padding='same', activation=None)\n",
    "    \n",
    "    def call(self, current_frame, ref_frame):\n",
    "        # In the real model, a multi-scale pyramid would be used.\n",
    "        # Here, we simply concatenate and pass through a series of conv layers.\n",
    "        x = tf.concat([current_frame, ref_frame], axis=-1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        flow = self.conv5(x)  # output: estimated motion (flow)\n",
    "        return flow\n",
    "\n",
    "# -------------------------------\n",
    "# Module 2: Motion Compression (Autoencoder)\n",
    "# -------------------------------\n",
    "class MotionCompressionAutoencoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MotionCompressionAutoencoder, self).__init__()\n",
    "        # Encoder: 4 conv layers with downsampling (×2 each time)\n",
    "        self.encoder_layers = [\n",
    "            tf.keras.layers.Conv2D(128, kernel_size=3, strides=2, padding='same'),\n",
    "            GDN(),\n",
    "            tf.keras.layers.Conv2D(128, kernel_size=3, strides=2, padding='same'),\n",
    "            GDN(),\n",
    "            tf.keras.layers.Conv2D(128, kernel_size=3, strides=2, padding='same'),\n",
    "            GDN(),\n",
    "            tf.keras.layers.Conv2D(128, kernel_size=3, strides=2, padding='same')\n",
    "        ]\n",
    "        # Decoder: 4 conv layers with upsampling (×2 each time)\n",
    "        self.decoder_layers = [\n",
    "            tf.keras.layers.Conv2DTranspose(128, kernel_size=3, strides=2, padding='same'),\n",
    "            IGDN(),\n",
    "            tf.keras.layers.Conv2DTranspose(128, kernel_size=3, strides=2, padding='same'),\n",
    "            IGDN(),\n",
    "            tf.keras.layers.Conv2DTranspose(128, kernel_size=3, strides=2, padding='same'),\n",
    "            IGDN(),\n",
    "            tf.keras.layers.Conv2DTranspose(2,   kernel_size=3, strides=2, padding='same')\n",
    "        ]\n",
    "    \n",
    "    def call(self, motion):\n",
    "        # Encode motion\n",
    "        x = motion\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        # Quantization could be added here (e.g., via rounding or learned quantization)\n",
    "        motion_latent = x\n",
    "        # Decode motion latent representation\n",
    "        x = motion_latent\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x)\n",
    "        # x approximates the compressed motion (flow)\n",
    "        return x, motion_latent  # return both for potential rate estimation\n",
    "\n",
    "# -------------------------------\n",
    "# Module 3: Motion Compensation Network\n",
    "# -------------------------------\n",
    "class MotionCompensationNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MotionCompensationNetwork, self).__init__()\n",
    "        # Architecture per Figure 2: series of conv layers with 3x3 filters.\n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, kernel_size=3, padding='same', activation='relu')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, kernel_size=3, padding='same', activation='relu')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(64, kernel_size=3, padding='same', activation='relu')\n",
    "        self.conv4 = tf.keras.layers.Conv2D(3,  kernel_size=3, padding='same', activation=None)\n",
    "    \n",
    "    def call(self, ref_frame, warped_frame, motion_comp):\n",
    "        # Concatenate the reference, warped reference and the motion information.\n",
    "        x = tf.concat([ref_frame, warped_frame, motion_comp], axis=-1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        compensated_frame = self.conv4(x)\n",
    "        return compensated_frame\n",
    "\n",
    "# -------------------------------\n",
    "# Module 4: Residual Compression (Autoencoder)\n",
    "# -------------------------------\n",
    "class ResidualCompressionAutoencoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ResidualCompressionAutoencoder, self).__init__()\n",
    "        # Similar structure as motion compression, but using 5x5 filters.\n",
    "        # Encoder: 4 conv layers with downsampling\n",
    "        self.encoder_layers = [\n",
    "            tf.keras.layers.Conv2D(128, kernel_size=5, strides=2, padding='same'),\n",
    "            GDN(),\n",
    "            tf.keras.layers.Conv2D(128, kernel_size=5, strides=2, padding='same'),\n",
    "            GDN(),\n",
    "            tf.keras.layers.Conv2D(128, kernel_size=5, strides=2, padding='same'),\n",
    "            GDN(),\n",
    "            tf.keras.layers.Conv2D(128, kernel_size=5, strides=2, padding='same')\n",
    "        ]\n",
    "        # Decoder: 4 conv layers with upsampling\n",
    "        self.decoder_layers = [\n",
    "            tf.keras.layers.Conv2DTranspose(128, kernel_size=5, strides=2, padding='same'),\n",
    "            IGDN(),\n",
    "            tf.keras.layers.Conv2DTranspose(128, kernel_size=5, strides=2, padding='same'),\n",
    "            IGDN(),\n",
    "            tf.keras.layers.Conv2DTranspose(128, kernel_size=5, strides=2, padding='same'),\n",
    "            IGDN(),\n",
    "            tf.keras.layers.Conv2DTranspose(3,   kernel_size=5, strides=2, padding='same')\n",
    "        ]\n",
    "    \n",
    "    def call(self, residual):\n",
    "        # Encode residual\n",
    "        x = residual\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        residual_latent = x\n",
    "        # Quantization could be added here too\n",
    "        # Decode latent residual representation\n",
    "        x = residual_latent\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x)\n",
    "        reconstructed_residual = x\n",
    "        return reconstructed_residual, residual_latent\n",
    "\n",
    "# -------------------------------\n",
    "# Full OpenDVC Model\n",
    "# -------------------------------\n",
    "class OpenDVCModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(OpenDVCModel, self).__init__()\n",
    "        self.motion_estimator = PyramidMotionEstimator()\n",
    "        self.motion_compressor = MotionCompressionAutoencoder()\n",
    "        self.motion_compensator = MotionCompensationNetwork()\n",
    "        self.residual_compressor = ResidualCompressionAutoencoder()\n",
    "    \n",
    "    def call(self, current_frame, ref_frame):\n",
    "        # 1. Estimate motion (optical flow) between current and previous frame.\n",
    "        estimated_flow = self.motion_estimator(current_frame, ref_frame)\n",
    "        \n",
    "        # 2. Compress the motion:\n",
    "        #    (i) Encode and (ii) decode (simulate quantization with a straight-through pass)\n",
    "        compressed_flow, motion_latent = self.motion_compressor(estimated_flow)\n",
    "        \n",
    "        # 3. Warp the reference frame using the (compressed) motion.\n",
    "        # Here we use a simple bilinear sampler provided by tf.contrib (or tf-addons) if available.\n",
    "        # For our example, we use tf.image.resize as a dummy warping (this is NOT real warping).\n",
    "        # In practice, use tf.contrib.image.dense_image_warp or equivalent.\n",
    "        warped_ref = tf.image.resize(ref_frame, tf.shape(ref_frame)[1:3])\n",
    "        \n",
    "        # 4. Motion Compensation: use the reference frame, warped frame and motion information.\n",
    "        compensated_frame = self.motion_compensator(ref_frame, warped_ref, compressed_flow)\n",
    "        \n",
    "        # 5. Compute the residual (difference between current frame and compensated frame).\n",
    "        residual = current_frame - compensated_frame\n",
    "        \n",
    "        # 6. Compress the residual.\n",
    "        reconstructed_residual, residual_latent = self.residual_compressor(residual)\n",
    "        \n",
    "        # 7. Reconstruct the final frame.\n",
    "        reconstructed_frame = compensated_frame + reconstructed_residual\n",
    "        \n",
    "        # For training, we would also output rate estimations (R) from the latents.\n",
    "        # Here we simply return latents as placeholders.\n",
    "        return {\n",
    "            'reconstructed_frame': reconstructed_frame,\n",
    "            'estimated_flow': estimated_flow,\n",
    "            'compressed_flow': compressed_flow,\n",
    "            'motion_latent': motion_latent,\n",
    "            'residual_latent': residual_latent\n",
    "        }\n",
    "\n",
    "# -------------------------------\n",
    "# Loss and Training Step (Simplified)\n",
    "# -------------------------------\n",
    "def compute_loss(current_frame, reconstructed_frame, motion_latent, residual_latent, lambda_rd=256):\n",
    "    # Distortion term: Mean Squared Error (MSE)\n",
    "    distortion = tf.reduce_mean(tf.square(current_frame - reconstructed_frame))\n",
    "    \n",
    "    # Rate term: we “simulate” the bit-rate loss by the L1 norm of the latents (placeholder)\n",
    "    rate_motion = tf.reduce_mean(tf.abs(motion_latent))\n",
    "    rate_residual = tf.reduce_mean(tf.abs(residual_latent))\n",
    "    rate_loss = rate_motion + rate_residual\n",
    "    \n",
    "    loss = lambda_rd * distortion + rate_loss\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, optimizer, current_frame, ref_frame, lambda_rd=256):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(current_frame, ref_frame)\n",
    "        loss = compute_loss(\n",
    "            current_frame,\n",
    "            outputs['reconstructed_frame'],\n",
    "            outputs['motion_latent'],\n",
    "            outputs['residual_latent'],\n",
    "            lambda_rd\n",
    "        )\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# -------------------------------\n",
    "# Example Usage\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the model and optimizer\n",
    "    model = OpenDVCModel()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    \n",
    "    # Dummy input: batch of 4 frames (height=256, width=256, channels=3)\n",
    "    current_frame = tf.random.normal([4, 256, 256, 3])\n",
    "    ref_frame = tf.random.normal([4, 256, 256, 3])\n",
    "    \n",
    "    # One training step (for demonstration; training loops need more bells and whistles)\n",
    "    loss = train_step(model, optimizer, current_frame, ref_frame)\n",
    "    print(\"Training loss:\", loss.numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
