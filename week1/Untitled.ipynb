{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d3833db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 17s 134ms/step - loss: 0.0330\n",
      "1/1 [==============================] - 1s 524ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Processing complete! Output saved as output.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ================== Memory-Saving Parameters ==================\n",
    "input_shape = (512, 512, 3)  # Reduced from 256x256\n",
    "batch_size = 4             # Reduced from 32\n",
    "latent_dim = 16              # More efficient compression\n",
    "epochs = 1                  # Reduced epochs for initial testing\n",
    "\n",
    "# ================== Data Generator ==================\n",
    "class FrameGenerator:\n",
    "    def __init__(self, video_path, batch_size):\n",
    "        self.cap = cv2.VideoCapture(video_path)\n",
    "        self.batch_size = batch_size\n",
    "        self.frame_count = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.frame_count // self.batch_size\n",
    "    \n",
    "    def __call__(self):\n",
    "        while True:\n",
    "            batch = []\n",
    "            for _ in range(self.batch_size):\n",
    "                ret, frame = self.cap.read()\n",
    "                if not ret:\n",
    "                    self.cap.release()\n",
    "                    return\n",
    "                frame = cv2.resize(frame, (input_shape[1], input_shape[0]))\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                batch.append(frame.astype(np.float32) / 255.0)\n",
    "            yield np.array(batch), np.array(batch)\n",
    "\n",
    "# ================== Simplified Autoencoder with Skip Connections ==================\n",
    "def build_autoencoder():\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    x1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x1)\n",
    "    x2 = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x2)\n",
    "    encoded = Conv2D(latent_dim, (3, 3), activation='relu', padding='same')(x)\n",
    "    \n",
    "    # Decoder with skip connections\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = concatenate([x, x2])  # Skip connection\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = concatenate([x, x1])  # Skip connection\n",
    "    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "    autoencoder = Model(inputs, decoded)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=1e-4), loss='mse')\n",
    "    return autoencoder\n",
    "\n",
    "# ================== Training with Generator ==================\n",
    "def main():\n",
    "    # Initialize generator\n",
    "    generator = FrameGenerator('input.mp4', batch_size)\n",
    "    \n",
    "    # Build and train model\n",
    "    autoencoder = build_autoencoder()\n",
    "    autoencoder.fit(\n",
    "        generator(),\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=len(generator),\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Reconstruct video\n",
    "    process_video(autoencoder, './input.mp4', './output.mp4')\n",
    "\n",
    "# ================== Memory-Efficient Video Processing ==================\n",
    "def process_video(model, input_path, output_path):\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, 30.0, (input_shape[1], input_shape[0]))\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Process frame\n",
    "        processed = cv2.resize(frame, (input_shape[1], input_shape[0]))\n",
    "        processed = cv2.cvtColor(processed, cv2.COLOR_BGR2RGB)\n",
    "        processed = np.expand_dims(processed, axis=0) / 255.0\n",
    "        \n",
    "        # Predict and write\n",
    "        reconstructed = model.predict(processed)[0]\n",
    "        reconstructed = (reconstructed * 255).astype(np.uint8)\n",
    "        out.write(cv2.cvtColor(reconstructed, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print(\"Processing complete! Output saved as output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f409314",
   "metadata": {},
   "source": [
    "# aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f56ba",
   "metadata": {},
   "source": [
    "# ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ec2a88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 374 frames and saved in './frames'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Video file name\n",
    "video_path = \"./input.mp4\"\n",
    "output_folder = \"./frames\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_number = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # Stop if no more frames\n",
    "\n",
    "    # Save frame as an image\n",
    "    frame_filename = os.path.join(output_folder, f\"frame_{frame_number:05d}.jpg\")\n",
    "    cv2.imwrite(frame_filename, frame)\n",
    "    \n",
    "    frame_number += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Extracted {frame_number} frames and saved in '{output_folder}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed4b63dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type                             | Params | Mode \n",
      "-----------------------------------------------------------------------\n",
      "0 | model     | CompressionAutoencoder           | 3.1 M  | train\n",
      "1 | criterion | MSELoss                          | 0      | train\n",
      "2 | psnr      | PeakSignalNoiseRatio             | 0      | train\n",
      "3 | ssim      | StructuralSimilarityIndexMeasure | 0      | train\n",
      "-----------------------------------------------------------------------\n",
      "3.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 M     Total params\n",
      "12.406    Total estimated model params size (MB)\n",
      "22        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571b230b88754f23a3a2a7a9a96999e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 33484, 29884, 30044, 29272) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1251\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1251\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 152\u001b[0m\n\u001b[0;32m    149\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mfit(model, datamodule\u001b[38;5;241m=\u001b[39mdata_module)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 152\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 149\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    133\u001b[0m checkpoint_callback \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\n\u001b[0;32m    134\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    135\u001b[0m     dirpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    139\u001b[0m )\n\u001b[0;32m    141\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[0;32m    142\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m    143\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[checkpoint_callback],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    147\u001b[0m )\n\u001b[1;32m--> 149\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    571\u001b[0m     ckpt_path,\n\u001b[0;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    574\u001b[0m )\n\u001b[1;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    987\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1024\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m   1023\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1024\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m   1026\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1053\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1050\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[1;32m-> 1053\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1055\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:137\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     batch, batch_idx, dataloader_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m previous_dataloader_idx \u001b[38;5;241m!=\u001b[39m dataloader_idx:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# the dataloader has changed, notify the logger connector\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_dataloader_outputs()\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:134\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:61\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_profiler()\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 61\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _ITERATOR_RETURN:\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n\u001b[0;32m    343\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:142\u001b[0m, in \u001b[0;36m_Sequential.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 142\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# try the next iterator\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_next_iterator()\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1458\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1458\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1420\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1416\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1419\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1420\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1421\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1422\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1264\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1263\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1265\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1266\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 33484, 29884, 30044, 29272) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.io import read_image\n",
    "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "class PaddedImageDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_files = [os.path.join(root_dir, f) for f in os.listdir(root_dir) \n",
    "                          if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        try:\n",
    "            # Use PIL to check if the image is valid\n",
    "            with Image.open(img_path) as img:\n",
    "                img.verify()  # Verify that the file is not corrupted\n",
    "            image = read_image(img_path).float() / 255.0\n",
    "            padded = F.pad(image, (0, 0, 0, 8), padding_mode='reflect')\n",
    "            return padded, image\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a dummy image or skip this file\n",
    "            dummy_image = torch.zeros((3, 1088, 1920))  # Adjust dimensions as needed\n",
    "            return dummy_image, dummy_image[:, :1080, :]\n",
    "\n",
    "class CompressionAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class AutoencoderSystem(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = CompressionAutoencoder()\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.psnr = PeakSignalNoiseRatio(data_range=1.0)\n",
    "        self.ssim = StructuralSimilarityIndexMeasure(data_range=1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self.model(inputs)\n",
    "        outputs_cropped = outputs[:, :, :1080, :]  # Remove padding\n",
    "        loss = self.criterion(outputs_cropped, targets)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self.model(inputs)\n",
    "        outputs_cropped = outputs[:, :, :1080, :]\n",
    "        \n",
    "        loss = self.criterion(outputs_cropped, targets)\n",
    "        psnr = self.psnr(outputs_cropped, targets)\n",
    "        ssim = self.ssim(outputs_cropped, targets)\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_psnr', psnr, prog_bar=True)\n",
    "        self.log('val_ssim', ssim, prog_bar=True)\n",
    "        return {'val_loss': loss, 'val_psnr': psnr, 'val_ssim': ssim}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "class ImageDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir='frames', batch_size=4):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        full_dataset = PaddedImageDataset(self.data_dir)\n",
    "        train_size = int(0.8 * len(full_dataset))\n",
    "        val_size = len(full_dataset) - train_size\n",
    "        self.train_dataset, self.val_dataset = random_split(\n",
    "            full_dataset, [train_size, val_size])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, \n",
    "                         shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size,\n",
    "                         shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "def main():\n",
    "    data_module = ImageDataModule(data_dir='./frames', batch_size=2)\n",
    "    \n",
    "    model = AutoencoderSystem()\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        dirpath='checkpoints',\n",
    "        filename='autoencoder-{epoch:02d}-{val_loss:.2f}',\n",
    "        save_top_k=3,\n",
    "        mode='min',\n",
    "    )\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        callbacks=[checkpoint_callback],\n",
    "        logger=TensorBoardLogger('logs/'),\n",
    "        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "        devices=1,\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff93ada6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU, compute capability 8.6\n",
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 512, 960, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 256, 480, 32)      896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 128, 240, 64)      18496     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 64, 120, 128)      73856     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 32, 60, 16)        18448     \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 64, 120, 128)     18560     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 128, 240, 64)     73792     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 256, 480, 32)     18464     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 512, 960, 3)      867       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 223,379\n",
      "Trainable params: 223,379\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "374/374 [==============================] - 12s 20ms/step - loss: 0.0000e+00\n",
      "Epoch 2/50\n",
      "374/374 [==============================] - 8s 20ms/step - loss: 0.0000e+00\n",
      "Epoch 3/50\n",
      "374/374 [==============================] - 8s 20ms/step - loss: 0.0000e+00\n",
      "Epoch 4/50\n",
      "374/374 [==============================] - 8s 20ms/step - loss: 0.0000e+00\n",
      "Epoch 5/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 6/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 7/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 8/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 9/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 10/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 11/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 12/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 13/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 14/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 15/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 16/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 17/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 18/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 19/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 20/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 21/50\n",
      "374/374 [==============================] - 8s 22ms/step - loss: 0.0000e+00\n",
      "Epoch 22/50\n",
      "374/374 [==============================] - 9s 23ms/step - loss: 0.0000e+00\n",
      "Epoch 23/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 24/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 25/50\n",
      "374/374 [==============================] - 8s 22ms/step - loss: 0.0000e+00\n",
      "Epoch 26/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 27/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 28/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 29/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 30/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 31/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 32/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 33/50\n",
      "374/374 [==============================] - 8s 22ms/step - loss: 0.0000e+00\n",
      "Epoch 34/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 35/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 36/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 37/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 38/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 39/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 40/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 41/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 42/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 43/50\n",
      "374/374 [==============================] - 8s 22ms/step - loss: 0.0000e+00\n",
      "Epoch 44/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 45/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 46/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 47/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 48/50\n",
      "374/374 [==============================] - 8s 21ms/step - loss: 0.0000e+00\n",
      "Epoch 49/50\n",
      "374/374 [==============================] - 8s 22ms/step - loss: 0.0000e+00\n",
      "Epoch 50/50\n",
      "374/374 [==============================] - 8s 22ms/step - loss: 0.0000e+00\n",
      "Model saved as 'autoencoder_model.h5'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "\n",
    "# Define a lighter autoencoder model\n",
    "def build_autoencoder(input_shape=(512, 960, 3)):  # Reduced resolution\n",
    "    \"\"\"\n",
    "    Builds a convolutional autoencoder with fewer filters for lower memory usage.\n",
    "    \n",
    "    Args:\n",
    "        input_shape (tuple): Shape of input images (height, width, channels).\n",
    "    \n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled autoencoder model.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    x = layers.Conv2D(32, (3, 3), strides=2, padding='same', activation='relu')(inputs)  # 256x480x32\n",
    "    x = layers.Conv2D(64, (3, 3), strides=2, padding='same', activation='relu')(x)      # 128x240x64\n",
    "    x = layers.Conv2D(128, (3, 3), strides=2, padding='same', activation='relu')(x)     # 64x120x128\n",
    "    latent = layers.Conv2D(16, (3, 3), strides=2, padding='same', activation='relu')(x)  # 32x60x16 (latent space)\n",
    "\n",
    "    # Decoder\n",
    "    x = layers.Conv2DTranspose(128, (3, 3), strides=2, padding='same', activation='relu')(latent)  # 64x120x128\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), strides=2, padding='same', activation='relu')(x)       # 128x240x64\n",
    "    x = layers.Conv2DTranspose(32, (3, 3), strides=2, padding='same', activation='relu')(x)       # 256x480x32\n",
    "    outputs = layers.Conv2DTranspose(3, (3, 3), strides=2, padding='same', activation='sigmoid')(x)  # 512x960x3\n",
    "\n",
    "    model = models.Model(inputs, outputs, name='autoencoder')\n",
    "    return model\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(folder_path, target_size=(512, 960), batch_size=1):\n",
    "    \"\"\"\n",
    "    Loads and resizes images from the folder.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing images.\n",
    "        target_size (tuple): Desired (height, width) for resized images.\n",
    "        batch_size (int): Number of images per batch.\n",
    "    \n",
    "    Returns:\n",
    "        tf.data.Dataset: Dataset yielding (input, target) pairs.\n",
    "    \"\"\"\n",
    "    file_paths = tf.data.Dataset.list_files(os.path.join(folder_path, '*.jpg'), shuffle=True)\n",
    "\n",
    "    def load_image(file_path):\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "        img = tf.image.resize(img, target_size, method='bilinear')  # Resize to target size\n",
    "        img = tf.cast(img, tf.float32) / 255.0  # Normalize to [0, 1]\n",
    "        return img\n",
    "\n",
    "    dataset = file_paths.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda x: (x, x), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Main execution\n",
    "if __name__ == '__main__':\n",
    "    # Parameters\n",
    "    INPUT_SHAPE = (512, 960, 3)  # Reduced resolution\n",
    "    BATCH_SIZE = 1  # Reduced to fit in memory\n",
    "    EPOCHS = 50\n",
    "    LEARNING_RATE = 1e-4\n",
    "    FOLDER_PATH = './frames'\n",
    "\n",
    "    # Enable mixed precision (optional, if supported)\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "    # Build and compile the model\n",
    "    autoencoder = build_autoencoder(input_shape=INPUT_SHAPE)\n",
    "    autoencoder.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='mse'\n",
    "    )\n",
    "    autoencoder.summary()\n",
    "\n",
    "    # Load the dataset\n",
    "    train_dataset = load_dataset(FOLDER_PATH, target_size=INPUT_SHAPE[:2], batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Train the model\n",
    "    try:\n",
    "        autoencoder.fit(\n",
    "            train_dataset,\n",
    "            epochs=EPOCHS,\n",
    "            verbose=1\n",
    "        )\n",
    "    except tf.errors.ResourceExhaustedError as e:\n",
    "        print(f\"OOM Error: {e}. Try reducing batch size or input resolution further.\")\n",
    "        exit(1)\n",
    "\n",
    "    # Save the model\n",
    "    autoencoder.save('autoencoder_model.h5')\n",
    "    print(\"Model saved as 'autoencoder_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "197c92a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from autoencoder_model.h5\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "Figure saved to reconstructions.png\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(folder_path, target_size=(512, 960), batch_size=1):\n",
    "    \"\"\"\n",
    "    Loads and resizes images from the folder for visualization.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing images.\n",
    "        target_size (tuple): Desired (height, width) for resized images.\n",
    "        batch_size (int): Number of images per batch.\n",
    "    \n",
    "    Returns:\n",
    "        tf.data.Dataset: Dataset yielding images.\n",
    "    \"\"\"\n",
    "    file_paths = tf.data.Dataset.list_files(os.path.join(folder_path, '*.jpg'), shuffle=True)\n",
    "\n",
    "    def load_image(file_path):\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "        img = tf.image.resize(img, target_size, method='bilinear')\n",
    "        img = tf.cast(img, tf.float32) / 255.0  # Normalize to [0, 1]\n",
    "        return img\n",
    "\n",
    "    dataset = file_paths.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Visualization function\n",
    "def visualize_reconstructions(model, dataset, num_images=3, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualizes original vs. reconstructed images.\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): Trained autoencoder model.\n",
    "        dataset (tf.data.Dataset): Dataset to draw images from.\n",
    "        num_images (int): Number of image pairs to display.\n",
    "        save_path (str, optional): Path to save the figure (e.g., 'reconstructions.png').\n",
    "    \"\"\"\n",
    "    # Take a batch from the dataset\n",
    "    for batch in dataset.take(1):\n",
    "        original_images = batch  # Single batch of images\n",
    "        reconstructed_images = model.predict(original_images)\n",
    "\n",
    "    # Convert to numpy and ensure float32 type, then clip values to [0, 1]\n",
    "    original_images = original_images.numpy().astype(np.float32)\n",
    "    reconstructed_images = np.clip(reconstructed_images.astype(np.float32), 0, 1)\n",
    "\n",
    "    # Plot original and reconstructed images\n",
    "    fig, axes = plt.subplots(2, num_images, figsize=(num_images * 5, 10))\n",
    "    for i in range(min(num_images, original_images.shape[0])):\n",
    "        # Original image\n",
    "        axes[0, i].imshow(original_images[i])\n",
    "        axes[0, i].set_title(f\"Original {i+1}\")\n",
    "        axes[0, i].axis('off')\n",
    "\n",
    "        # Reconstructed image\n",
    "        axes[1, i].imshow(reconstructed_images[i])\n",
    "        axes[1, i].set_title(f\"Reconstructed {i+1}\")\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure if a path is provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "# Main execution\n",
    "if __name__ == '__main__':\n",
    "    # Parameters\n",
    "    INPUT_SHAPE = (512, 960, 3)  # Must match the model's input shape\n",
    "    FOLDER_PATH = 'frames'       # Folder with original images\n",
    "    MODEL_PATH = 'autoencoder_model.h5'  # Path to the trained model\n",
    "    NUM_IMAGES = 3               # Number of images to visualize\n",
    "    SAVE_PATH = 'reconstructions.png'  # Optional: save the figure\n",
    "\n",
    "    # Load the trained model\n",
    "    try:\n",
    "        autoencoder = tf.keras.models.load_model(MODEL_PATH)\n",
    "        print(f\"Model loaded from {MODEL_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # Load the dataset\n",
    "    viz_dataset = load_dataset(FOLDER_PATH, target_size=INPUT_SHAPE[:2], batch_size=NUM_IMAGES)\n",
    "\n",
    "    # Visualize original vs reconstructed images\n",
    "    visualize_reconstructions(autoencoder, viz_dataset, num_images=NUM_IMAGES, save_path=SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebf8c671",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 9.27 GiB for an array with shape (50, 24883200) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m subset_frames \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(subset_frames)  \u001b[38;5;66;03m# Shape: (subset_size, 1080*1920*3)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39mn_components)\n\u001b[1;32m---> 34\u001b[0m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset_frames\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Fit PCA on the subset\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Step 2: Process each frame one at a time\u001b[39;00m\n\u001b[0;32m     37\u001b[0m height, width, channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1080\u001b[39m, \u001b[38;5;241m1920\u001b[39m, \u001b[38;5;241m3\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:442\u001b[0m, in \u001b[0;36mPCA.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    426\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X.\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \n\u001b[0;32m    428\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;124;03m        Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:542\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;66;03m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovariance_eigh\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_array_api_compliant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_truncated(X, n_components, xp)\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:571\u001b[0m, in \u001b[0;36mPCA._fit_full\u001b[1;34m(self, X, n_components, xp, is_array_api_compliant)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_ \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mreshape(xp\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_), (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,))\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 571\u001b[0m     X_centered \u001b[38;5;241m=\u001b[39m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[0;32m    572\u001b[0m     X_centered \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_\n\u001b[0;32m    573\u001b[0m     x_is_centered \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\utils\\_array_api.py:405\u001b[0m, in \u001b[0;36m_NumPyAPIWrapper.asarray\u001b[1;34m(self, x, dtype, device, copy)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;66;03m# Support copy in NumPy namespace\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39masarray(x, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 9.27 GiB for an array with shape (50, 24883200) and data type float64"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Parameters\n",
    "input_folder = \"./frames\"\n",
    "output_folder = \"compressed_frames\"\n",
    "output_video = \"output_video.mp4\"\n",
    "n_components = 50  # Number of PCA components\n",
    "subset_size = 50   # Number of frames to fit PCA (must be >= n_components)\n",
    "frame_size = (1920, 1080)\n",
    "fps = 30\n",
    "\n",
    "# Create output folder if it doesn’t exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Load image files\n",
    "image_files = sorted([f for f in os.listdir(input_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "n_frames = len(image_files)\n",
    "\n",
    "# Step 1: Fit PCA on a subset of frames\n",
    "subset_frames = []\n",
    "for i in range(min(subset_size, n_frames)):\n",
    "    img_path = os.path.join(input_folder, image_files[i])\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_array = np.array(img).flatten()  # Shape: (1080*1920*3,)\n",
    "    subset_frames.append(img_array)\n",
    "\n",
    "subset_frames = np.array(subset_frames)  # Shape: (subset_size, 1080*1920*3)\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(subset_frames)  # Fit PCA on the subset\n",
    "\n",
    "# Step 2: Process each frame one at a time\n",
    "height, width, channels = 1080, 1920, 3\n",
    "for i, file in enumerate(image_files):\n",
    "    # Load frame\n",
    "    img_path = os.path.join(input_folder, file)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_array = np.array(img)  # Shape: (1080, 1920, 3)\n",
    "    \n",
    "    # Flatten frame\n",
    "    flattened_frame = img_array.reshape(1, height * width * channels)  # Shape: (1, 1080*1920*3)\n",
    "    \n",
    "    # Apply PCA\n",
    "    compressed_frame = pca.transform(flattened_frame)  # Compress\n",
    "    reconstructed_frame = pca.inverse_transform(compressed_frame)  # Reconstruct\n",
    "    \n",
    "    # Reshape and clip\n",
    "    reconstructed_frame = reconstructed_frame.reshape(height, width, channels)\n",
    "    reconstructed_frame = np.clip(reconstructed_frame, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    # Save reconstructed frame\n",
    "    output_path = os.path.join(output_folder, f\"frame_{i:04d}.png\")\n",
    "    Image.fromarray(reconstructed_frame).save(output_path)\n",
    "    \n",
    "    print(f\"Processed frame {i+1}/{n_frames}\")\n",
    "\n",
    "# Step 3: Create MP4 video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "video_writer = cv2.VideoWriter(output_video, fourcc, fps, frame_size)\n",
    "\n",
    "for i in range(n_frames):\n",
    "    frame_path = os.path.join(output_folder, f\"frame_{i:04d}.png\")\n",
    "    frame = cv2.imread(frame_path)\n",
    "    video_writer.write(frame)\n",
    "\n",
    "video_writer.release()\n",
    "print(f\"Video saved as {output_video}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43819519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 374 frames from 'input.mp4'...\n",
      "Processing complete. Output saved to 'output.mp4'.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def process_video():\n",
    "    # Hardcoded file names\n",
    "    input_file = \"input.mp4\"\n",
    "    output_file = \"output.mp4\"\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_file)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open\", input_file)\n",
    "        return\n",
    "    \n",
    "    # Get video properties.\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_file, fourcc, fps, (width, height))\n",
    "    \n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Processing {frame_count} frames from '{input_file}'...\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert from BGR to YUV.\n",
    "        yuv = cv2.cvtColor(frame, cv2.COLOR_BGR2YUV)\n",
    "        y, u, v = cv2.split(yuv)\n",
    "        \n",
    "        # Reduce chrominance details by downscaling U and V channels\n",
    "        u_small = cv2.resize(u, (width // 4, height // 4), interpolation=cv2.INTER_LINEAR)\n",
    "        v_small = cv2.resize(v, (width // 4, height // 4), interpolation=cv2.INTER_LINEAR)\n",
    "        u_restored = cv2.resize(u_small, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "        v_restored = cv2.resize(v_small, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # Merge the original luminance with the degraded chrominance channels.\n",
    "        yuv_modified = cv2.merge([y, u_restored, v_restored])\n",
    "        \n",
    "        # Convert back to BGR and write the frame.\n",
    "        frame_modified = cv2.cvtColor(yuv_modified, cv2.COLOR_YUV2BGR)\n",
    "        out.write(frame_modified)\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Processing complete. Output saved to '{output_file}'.\")\n",
    "\n",
    "# Run the video processing function.\n",
    "process_video()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "882270eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 374 frames with simulated compression...\n",
      "Processing complete. Output saved to 'output.mp4'.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def process_video_compression():\n",
    "    input_file = \"input.mp4\"\n",
    "    output_file = \"output.mp4\"\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_file)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open\", input_file)\n",
    "        return\n",
    "    \n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Using mp4v codec; adjust if needed.\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_file, fourcc, fps, (width, height))\n",
    "    \n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Processing {frame_count} frames with simulated compression...\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Compress the frame to JPEG format at low quality (simulate compression)\n",
    "        ret_enc, compressed = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 30])\n",
    "        if not ret_enc:\n",
    "            print(\"Frame compression failed, skipping frame...\")\n",
    "            continue\n",
    "        \n",
    "        # Reconstruct the frame by decoding it back\n",
    "        frame_reconstructed = cv2.imdecode(compressed, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        # Write the reconstructed frame to the output video.\n",
    "        out.write(frame_reconstructed)\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Processing complete. Output saved to '{output_file}'.\")\n",
    "\n",
    "process_video_compression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0115f7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video into latent space. Total frames: 374\n",
      "Latent representations saved to latents.npy\n",
      "Latent representation file size: 48000 bytes\n",
      "Reconstructing video from latent representations. Total frames: 374\n",
      "Reconstructed video saved to reconstructed_output.mp4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define a simple VAE model for 64x64 RGB images.\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc_conv1 = nn.Conv2d(3, 16, 4, stride=2, padding=1)   # 64->32\n",
    "        self.enc_conv2 = nn.Conv2d(16, 32, 4, stride=2, padding=1)  # 32->16\n",
    "        self.enc_conv3 = nn.Conv2d(32, 64, 4, stride=2, padding=1)  # 16->8\n",
    "        self.enc_conv4 = nn.Conv2d(64, 128, 4, stride=2, padding=1) # 8->4\n",
    "        \n",
    "        # Fully connected layers for mu and logvar (flattened feature size = 128*4*4 = 2048)\n",
    "        self.fc_mu = nn.Linear(128 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128 * 4 * 4, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc_dec = nn.Linear(latent_dim, 128 * 4 * 4)\n",
    "        self.dec_conv1 = nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1)  # 4->8\n",
    "        self.dec_conv2 = nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1)   # 8->16\n",
    "        self.dec_conv3 = nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1)   # 16->32\n",
    "        self.dec_conv4 = nn.ConvTranspose2d(16, 3, 4, stride=2, padding=1)    # 32->64\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.enc_conv1(x))\n",
    "        h = F.relu(self.enc_conv2(h))\n",
    "        h = F.relu(self.enc_conv3(h))\n",
    "        h = F.relu(self.enc_conv4(h))\n",
    "        h = h.view(h.size(0), -1)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc_dec(z))\n",
    "        h = h.view(-1, 128, 4, 4)\n",
    "        h = F.relu(self.dec_conv1(h))\n",
    "        h = F.relu(self.dec_conv2(h))\n",
    "        h = F.relu(self.dec_conv3(h))\n",
    "        h = torch.sigmoid(self.dec_conv4(h))\n",
    "        return h\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "# Function to convert video frames to latent representations and save them.\n",
    "def video_to_latent(vae, video_path, latent_save_path, frame_size=(64, 64), device='cpu'):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Cannot open video file:\", video_path)\n",
    "        return\n",
    "    latents = []\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"Processing video into latent space. Total frames:\", frame_count)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Resize frame and convert from BGR to RGB.\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_resized = cv2.resize(frame_rgb, frame_size)\n",
    "        \n",
    "        # Convert to tensor and normalize.\n",
    "        frame_tensor = torch.from_numpy(frame_resized).permute(2, 0, 1).float() / 255.0\n",
    "        frame_tensor = frame_tensor.unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get latent representation (using mu).\n",
    "        with torch.no_grad():\n",
    "            mu, _ = vae.encode(frame_tensor)\n",
    "        latents.append(mu.cpu().numpy())\n",
    "    \n",
    "    cap.release()\n",
    "    latents = np.concatenate(latents, axis=0)  # shape: (num_frames, latent_dim)\n",
    "    np.save(latent_save_path, latents)\n",
    "    print(\"Latent representations saved to\", latent_save_path)\n",
    "    return latents\n",
    "\n",
    "# Function to reconstruct video from saved latent representations.\n",
    "def latent_to_video(vae, latent_file, output_video_path, frame_size=(64, 64), fps=25, device='cpu'):\n",
    "    latents = np.load(latent_file)\n",
    "    num_frames = latents.shape[0]\n",
    "    print(\"Reconstructing video from latent representations. Total frames:\", num_frames)\n",
    "    \n",
    "    # Set up the VideoWriter.\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, frame_size)\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        latent = torch.tensor(latents[i:i+1], dtype=torch.float).to(device)\n",
    "        with torch.no_grad():\n",
    "            recon = vae.decode(latent)\n",
    "        # Convert tensor to image format.\n",
    "        recon_img = recon.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255\n",
    "        recon_img = np.clip(recon_img, 0, 255).astype(np.uint8)\n",
    "        # Convert RGB back to BGR.\n",
    "        recon_img = cv2.cvtColor(recon_img, cv2.COLOR_RGB2BGR)\n",
    "        out.write(recon_img)\n",
    "    out.release()\n",
    "    print(\"Reconstructed video saved to\", output_video_path)\n",
    "\n",
    "# Example usage.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vae = VAE(latent_dim=32).to(device)\n",
    "\n",
    "# NOTE: In a real project, you must train your VAE on video frames first.\n",
    "# For this demonstration, we're using an untrained model (which means quality won't be good).\n",
    "\n",
    "# Compress video into latent space.\n",
    "latent_file = \"latents.npy\"\n",
    "video_to_latent(vae, \"input.mp4\", latent_file, frame_size=(64, 64), device=device)\n",
    "\n",
    "# Check the file size of the latent representations.\n",
    "latent_size = os.path.getsize(latent_file)\n",
    "print(\"Latent representation file size:\", latent_size, \"bytes\")\n",
    "\n",
    "# Reconstruct video from the latent representations.\n",
    "latent_to_video(vae, latent_file, \"reconstructed_output.mp4\", frame_size=(64, 64), fps=25, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5878e094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/20, Loss: 707.8670\n",
      "Epoch 2/20, Loss: 551.4452\n",
      "Epoch 3/20, Loss: 492.9435\n",
      "Epoch 4/20, Loss: 446.2162\n",
      "Epoch 5/20, Loss: 427.1008\n",
      "Epoch 6/20, Loss: 421.1564\n",
      "Epoch 7/20, Loss: 415.6964\n",
      "Epoch 8/20, Loss: 405.6358\n",
      "Epoch 9/20, Loss: 396.7384\n",
      "Epoch 10/20, Loss: 383.3513\n",
      "Epoch 11/20, Loss: 348.4491\n",
      "Epoch 12/20, Loss: 329.1100\n",
      "Epoch 13/20, Loss: 309.2149\n",
      "Epoch 14/20, Loss: 291.4519\n",
      "Epoch 15/20, Loss: 267.1868\n",
      "Epoch 16/20, Loss: 241.2050\n",
      "Epoch 17/20, Loss: 215.9315\n",
      "Epoch 18/20, Loss: 193.8725\n",
      "Epoch 19/20, Loss: 179.9552\n",
      "Epoch 20/20, Loss: 167.3749\n",
      "Model saved to vae_model.pth\n",
      "Encoding video into latent space. Total frames: 374\n",
      "Latent representations saved to latents.npy\n",
      "Latent representation file size: 48000 bytes\n",
      "Reconstructing video from latent representations. Total frames: 374\n",
      "Reconstructed video saved to reconstructed_output.mp4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "# Define the VAE architecture.\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder: downscaling from 64x64 to 4x4 feature maps.\n",
    "        self.enc_conv1 = nn.Conv2d(3, 16, 4, stride=2, padding=1)   # 64->32\n",
    "        self.enc_conv2 = nn.Conv2d(16, 32, 4, stride=2, padding=1)  # 32->16\n",
    "        self.enc_conv3 = nn.Conv2d(32, 64, 4, stride=2, padding=1)  # 16->8\n",
    "        self.enc_conv4 = nn.Conv2d(64, 128, 4, stride=2, padding=1) # 8->4\n",
    "        \n",
    "        # Fully connected layers to produce mu and logvar.\n",
    "        self.fc_mu = nn.Linear(128 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128 * 4 * 4, latent_dim)\n",
    "        \n",
    "        # Decoder: from latent vector back to image.\n",
    "        self.fc_dec = nn.Linear(latent_dim, 128 * 4 * 4)\n",
    "        self.dec_conv1 = nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1)  # 4->8\n",
    "        self.dec_conv2 = nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1)   # 8->16\n",
    "        self.dec_conv3 = nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1)   # 16->32\n",
    "        self.dec_conv4 = nn.ConvTranspose2d(16, 3, 4, stride=2, padding=1)    # 32->64\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.enc_conv1(x))\n",
    "        h = F.relu(self.enc_conv2(h))\n",
    "        h = F.relu(self.enc_conv3(h))\n",
    "        h = F.relu(self.enc_conv4(h))\n",
    "        h = h.view(h.size(0), -1)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc_dec(z))\n",
    "        h = h.view(-1, 128, 4, 4)\n",
    "        h = F.relu(self.dec_conv1(h))\n",
    "        h = F.relu(self.dec_conv2(h))\n",
    "        h = F.relu(self.dec_conv3(h))\n",
    "        h = torch.sigmoid(self.dec_conv4(h))\n",
    "        return h\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "# Create a custom dataset that extracts frames from the video.\n",
    "class VideoFrameDataset(Dataset):\n",
    "    def __init__(self, video_path, frame_size=(64, 64)):\n",
    "        self.frames = []\n",
    "        self.frame_size = frame_size\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(\"Cannot open video file: \" + video_path)\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # Convert BGR to RGB and resize.\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_resized = cv2.resize(frame_rgb, self.frame_size)\n",
    "            # Normalize to [0, 1].\n",
    "            frame_normalized = frame_resized.astype(np.float32) / 255.0\n",
    "            self.frames.append(frame_normalized)\n",
    "        cap.release()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        frame = self.frames[idx]\n",
    "        frame_tensor = torch.from_numpy(frame).permute(2, 0, 1)  # (H, W, C) -> (C, H, W)\n",
    "        return frame_tensor\n",
    "\n",
    "# Define the VAE loss: reconstruction (MSE) + KL divergence.\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # Reconstruction loss (sum over all pixels).\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    # KL divergence loss.\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_loss\n",
    "\n",
    "# Training loop for the VAE.\n",
    "def train_vae(vae, dataloader, num_epochs=20, learning_rate=1e-3, device='cpu'):\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "    vae.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = vae(batch)\n",
    "            loss = loss_function(recon_batch, batch, mu, logvar)\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Convert the video into latent representations (using the encoder) and save them.\n",
    "def video_to_latent(vae, video_path, latent_save_path, frame_size=(64, 64), device='cpu'):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Cannot open video file:\", video_path)\n",
    "        return\n",
    "    latents = []\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"Encoding video into latent space. Total frames:\", frame_count)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_resized = cv2.resize(frame_rgb, frame_size)\n",
    "        frame_tensor = torch.from_numpy(frame_resized).permute(2, 0, 1).float() / 255.0\n",
    "        frame_tensor = frame_tensor.unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            mu, _ = vae.encode(frame_tensor)\n",
    "        latents.append(mu.cpu().numpy())\n",
    "    cap.release()\n",
    "    latents = np.concatenate(latents, axis=0)  # Shape: (num_frames, latent_dim)\n",
    "    np.save(latent_save_path, latents)\n",
    "    print(\"Latent representations saved to\", latent_save_path)\n",
    "    return latents\n",
    "\n",
    "# Reconstruct a video from the saved latent representations.\n",
    "def latent_to_video(vae, latent_file, output_video_path, frame_size=(64, 64), fps=25, device='cpu'):\n",
    "    latents = np.load(latent_file)\n",
    "    num_frames = latents.shape[0]\n",
    "    print(\"Reconstructing video from latent representations. Total frames:\", num_frames)\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, frame_size)\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        latent = torch.tensor(latents[i:i+1], dtype=torch.float).to(device)\n",
    "        with torch.no_grad():\n",
    "            recon = vae.decode(latent)\n",
    "        # Convert tensor to image.\n",
    "        recon_img = recon.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255\n",
    "        recon_img = np.clip(recon_img, 0, 255).astype(np.uint8)\n",
    "        # Convert RGB back to BGR.\n",
    "        recon_img = cv2.cvtColor(recon_img, cv2.COLOR_RGB2BGR)\n",
    "        out.write(recon_img)\n",
    "    out.release()\n",
    "    print(\"Reconstructed video saved to\", output_video_path)\n",
    "\n",
    "# Main execution: training and inference.\n",
    "if __name__ == \"__main__\":\n",
    "    # Choose device.\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    vae = VAE(latent_dim=32).to(device)\n",
    "    \n",
    "    # Create dataset and dataloader from the video frames.\n",
    "    dataset = VideoFrameDataset(\"input.mp4\", frame_size=(64, 64))\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Train the VAE.\n",
    "    print(\"Starting training...\")\n",
    "    train_vae(vae, dataloader, num_epochs=20, learning_rate=1e-3, device=device)\n",
    "    \n",
    "    # Optionally, save the trained model.\n",
    "    torch.save(vae.state_dict(), \"vae_model.pth\")\n",
    "    print(\"Model saved to vae_model.pth\")\n",
    "    \n",
    "    # Convert the video into latent representations and save to disk.\n",
    "    latent_file = \"latents.npy\"\n",
    "    video_to_latent(vae, \"input.mp4\", latent_file, frame_size=(64, 64), device=device)\n",
    "    latent_size = os.path.getsize(latent_file)\n",
    "    print(\"Latent representation file size:\", latent_size, \"bytes\")\n",
    "    \n",
    "    # Reconstruct the video from the latent representations.\n",
    "    latent_to_video(vae, latent_file, \"reconstructed_output.mp4\", frame_size=(64, 64), fps=25, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dda51b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading dataset...\n",
      "Loading video: input.mp4\n",
      "Loaded 100 frames...\n",
      "Loaded 200 frames...\n",
      "Loaded 300 frames...\n",
      "Finished loading. Total frames: 374\n",
      "Starting training...\n",
      "Starting epoch 1/50\n",
      "Epoch 1, Batch 0, Loss: 300004.8438\n",
      "Epoch 1, Batch 10, Loss: 267183.8438\n",
      "Epoch 1/50, Loss: 8743.9752, LR: 3.00e-04\n",
      "Starting epoch 2/50\n",
      "Epoch 2, Batch 0, Loss: 264572.5625\n",
      "Epoch 2, Batch 10, Loss: 250934.7031\n",
      "Epoch 2/50, Loss: 8022.7605, LR: 3.00e-04\n",
      "Starting epoch 3/50\n",
      "Epoch 3, Batch 0, Loss: 249717.4688\n",
      "Epoch 3, Batch 10, Loss: 243038.8594\n",
      "Epoch 3/50, Loss: 7683.5009, LR: 3.00e-04\n",
      "Starting epoch 4/50\n",
      "Epoch 4, Batch 0, Loss: 242437.8125\n",
      "Epoch 4, Batch 10, Loss: 240148.0156\n",
      "Epoch 4/50, Loss: 7509.6561, LR: 3.00e-04\n",
      "Starting epoch 5/50\n",
      "Epoch 5, Batch 0, Loss: 238242.6875\n",
      "Epoch 5, Batch 10, Loss: 235813.5625\n",
      "Epoch 5/50, Loss: 7415.5092, LR: 3.00e-04\n",
      "Starting epoch 6/50\n",
      "Epoch 6, Batch 0, Loss: 235611.7500\n",
      "Epoch 6, Batch 10, Loss: 235423.3281\n",
      "Epoch 6/50, Loss: 7359.9569, LR: 3.00e-04\n",
      "Starting epoch 7/50\n",
      "Epoch 7, Batch 0, Loss: 235060.7656\n",
      "Epoch 7, Batch 10, Loss: 234081.3906\n",
      "Epoch 7/50, Loss: 7321.2726, LR: 3.00e-04\n",
      "Starting epoch 8/50\n",
      "Epoch 8, Batch 0, Loss: 233717.3125\n",
      "Epoch 8, Batch 10, Loss: 232740.5156\n",
      "Epoch 8/50, Loss: 7294.7303, LR: 3.00e-04\n",
      "Starting epoch 9/50\n",
      "Epoch 9, Batch 0, Loss: 234107.7031\n",
      "Epoch 9, Batch 10, Loss: 230672.1094\n",
      "Epoch 9/50, Loss: 7267.6346, LR: 3.00e-04\n",
      "Starting epoch 10/50\n",
      "Epoch 10, Batch 0, Loss: 233450.4375\n",
      "Epoch 10, Batch 10, Loss: 229871.2656\n",
      "Epoch 10/50, Loss: 7248.1368, LR: 3.00e-04\n",
      "Starting epoch 11/50\n",
      "Epoch 11, Batch 0, Loss: 232287.9844\n",
      "Epoch 11, Batch 10, Loss: 233597.1406\n",
      "Epoch 11/50, Loss: 7237.0506, LR: 3.00e-04\n",
      "Starting epoch 12/50\n",
      "Epoch 12, Batch 0, Loss: 232661.3750\n",
      "Epoch 12, Batch 10, Loss: 229320.3125\n",
      "Epoch 12/50, Loss: 7221.9273, LR: 3.00e-04\n",
      "Starting epoch 13/50\n",
      "Epoch 13, Batch 0, Loss: 230496.5469\n",
      "Epoch 13, Batch 10, Loss: 229887.0781\n",
      "Epoch 13/50, Loss: 7207.9078, LR: 3.00e-04\n",
      "Starting epoch 14/50\n",
      "Epoch 14, Batch 0, Loss: 231118.3438\n",
      "Epoch 14, Batch 10, Loss: 230141.0156\n",
      "Epoch 14/50, Loss: 7193.8968, LR: 3.00e-04\n",
      "Starting epoch 15/50\n",
      "Epoch 15, Batch 0, Loss: 230036.9062\n",
      "Epoch 15, Batch 10, Loss: 230007.8125\n",
      "Epoch 15/50, Loss: 7182.6258, LR: 3.00e-04\n",
      "Starting epoch 16/50\n",
      "Epoch 16, Batch 0, Loss: 230424.6406\n",
      "Epoch 16, Batch 10, Loss: 229614.4375\n",
      "Epoch 16/50, Loss: 7172.2312, LR: 3.00e-04\n",
      "Starting epoch 17/50\n",
      "Epoch 17, Batch 0, Loss: 230416.0625\n",
      "Epoch 17, Batch 10, Loss: 228799.5312\n",
      "Epoch 17/50, Loss: 7164.2765, LR: 3.00e-04\n",
      "Starting epoch 18/50\n",
      "Epoch 18, Batch 0, Loss: 229223.0156\n",
      "Epoch 18, Batch 10, Loss: 230185.6719\n",
      "Epoch 18/50, Loss: 7157.7626, LR: 3.00e-04\n",
      "Starting epoch 19/50\n",
      "Epoch 19, Batch 0, Loss: 228695.5469\n",
      "Epoch 19, Batch 10, Loss: 228059.9844\n",
      "Epoch 19/50, Loss: 7149.1557, LR: 3.00e-04\n",
      "Starting epoch 20/50\n",
      "Epoch 20, Batch 0, Loss: 228209.8438\n",
      "Epoch 20, Batch 10, Loss: 227568.6250\n",
      "Epoch 20/50, Loss: 7143.3797, LR: 3.00e-04\n",
      "Starting epoch 21/50\n",
      "Epoch 21, Batch 0, Loss: 227182.0000\n",
      "Epoch 21, Batch 10, Loss: 228552.7344\n",
      "Epoch 21/50, Loss: 7137.2360, LR: 3.00e-04\n",
      "Starting epoch 22/50\n",
      "Epoch 22, Batch 0, Loss: 228297.4375\n",
      "Epoch 22, Batch 10, Loss: 227079.7188\n",
      "Epoch 22/50, Loss: 7130.1275, LR: 3.00e-04\n",
      "Starting epoch 23/50\n",
      "Epoch 23, Batch 0, Loss: 229704.2500\n",
      "Epoch 23, Batch 10, Loss: 226586.1562\n",
      "Epoch 23/50, Loss: 7126.7229, LR: 3.00e-04\n",
      "Starting epoch 24/50\n",
      "Epoch 24, Batch 0, Loss: 228809.1094\n",
      "Epoch 24, Batch 10, Loss: 228434.4531\n",
      "Epoch 24/50, Loss: 7120.4043, LR: 3.00e-04\n",
      "Starting epoch 25/50\n",
      "Epoch 25, Batch 0, Loss: 228468.0781\n",
      "Epoch 25, Batch 10, Loss: 226496.7812\n",
      "Epoch 25/50, Loss: 7124.7151, LR: 3.00e-04\n",
      "Starting epoch 26/50\n",
      "Epoch 26, Batch 0, Loss: 228299.0781\n",
      "Epoch 26, Batch 10, Loss: 229889.3594\n",
      "Epoch 26/50, Loss: 7112.8153, LR: 3.00e-04\n",
      "Starting epoch 27/50\n",
      "Epoch 27, Batch 0, Loss: 227209.2188\n",
      "Epoch 27, Batch 10, Loss: 228011.6875\n",
      "Epoch 27/50, Loss: 7106.4534, LR: 3.00e-04\n",
      "Starting epoch 28/50\n",
      "Epoch 28, Batch 0, Loss: 227185.2656\n",
      "Epoch 28, Batch 10, Loss: 228051.6406\n",
      "Epoch 28/50, Loss: 7105.3197, LR: 3.00e-04\n",
      "Starting epoch 29/50\n",
      "Epoch 29, Batch 0, Loss: 227712.7500\n",
      "Epoch 29, Batch 10, Loss: 228706.8281\n",
      "Epoch 29/50, Loss: 7098.5658, LR: 3.00e-04\n",
      "Starting epoch 30/50\n",
      "Epoch 30, Batch 0, Loss: 227853.1406\n",
      "Epoch 30, Batch 10, Loss: 226524.9844\n",
      "Epoch 30/50, Loss: 7094.9226, LR: 3.00e-04\n",
      "Starting epoch 31/50\n",
      "Epoch 31, Batch 0, Loss: 226500.5625\n",
      "Epoch 31, Batch 10, Loss: 226983.3594\n",
      "Epoch 31/50, Loss: 7093.5211, LR: 3.00e-04\n",
      "Starting epoch 32/50\n",
      "Epoch 32, Batch 0, Loss: 227493.5156\n",
      "Epoch 32, Batch 10, Loss: 226959.8281\n",
      "Epoch 32/50, Loss: 7093.3791, LR: 3.00e-04\n",
      "Starting epoch 33/50\n",
      "Epoch 33, Batch 0, Loss: 227867.5625\n",
      "Epoch 33, Batch 10, Loss: 227662.8125\n",
      "Epoch 33/50, Loss: 7091.2882, LR: 3.00e-04\n",
      "Starting epoch 34/50\n",
      "Epoch 34, Batch 0, Loss: 226115.3438\n",
      "Epoch 34, Batch 10, Loss: 227754.9531\n",
      "Epoch 34/50, Loss: 7088.9880, LR: 3.00e-04\n",
      "Starting epoch 35/50\n",
      "Epoch 35, Batch 0, Loss: 226287.6406\n",
      "Epoch 35, Batch 10, Loss: 227050.6250\n",
      "Epoch 35/50, Loss: 7084.1862, LR: 3.00e-04\n",
      "Starting epoch 36/50\n",
      "Epoch 36, Batch 0, Loss: 225995.3281\n",
      "Epoch 36, Batch 10, Loss: 228063.1875\n",
      "Epoch 36/50, Loss: 7083.4383, LR: 3.00e-04\n",
      "Starting epoch 37/50\n",
      "Epoch 37, Batch 0, Loss: 226601.8281\n",
      "Epoch 37, Batch 10, Loss: 225851.0000\n",
      "Epoch 37/50, Loss: 7076.0964, LR: 3.00e-04\n",
      "Starting epoch 38/50\n",
      "Epoch 38, Batch 0, Loss: 226777.5156\n",
      "Epoch 38, Batch 10, Loss: 227668.9531\n",
      "Epoch 38/50, Loss: 7074.9113, LR: 3.00e-04\n",
      "Starting epoch 39/50\n",
      "Epoch 39, Batch 0, Loss: 225783.5156\n",
      "Epoch 39, Batch 10, Loss: 226929.7969\n",
      "Epoch 39/50, Loss: 7071.6487, LR: 3.00e-04\n",
      "Starting epoch 40/50\n",
      "Epoch 40, Batch 0, Loss: 225049.6562\n",
      "Epoch 40, Batch 10, Loss: 226285.6562\n",
      "Epoch 40/50, Loss: 7070.2137, LR: 3.00e-04\n",
      "Starting epoch 41/50\n",
      "Epoch 41, Batch 0, Loss: 228132.3125\n",
      "Epoch 41, Batch 10, Loss: 225179.9375\n",
      "Epoch 41/50, Loss: 7069.0206, LR: 3.00e-04\n",
      "Starting epoch 42/50\n",
      "Epoch 42, Batch 0, Loss: 227934.0938\n",
      "Epoch 42, Batch 10, Loss: 225557.7344\n",
      "Epoch 42/50, Loss: 7069.0800, LR: 3.00e-04\n",
      "Starting epoch 43/50\n",
      "Epoch 43, Batch 0, Loss: 225820.4688\n",
      "Epoch 43, Batch 10, Loss: 225684.4844\n",
      "Epoch 43/50, Loss: 7065.6890, LR: 3.00e-04\n",
      "Starting epoch 44/50\n",
      "Epoch 44, Batch 0, Loss: 224471.8906\n",
      "Epoch 44, Batch 10, Loss: 226953.4531\n",
      "Epoch 44/50, Loss: 7068.5288, LR: 3.00e-04\n",
      "Starting epoch 45/50\n",
      "Epoch 45, Batch 0, Loss: 225880.1719\n",
      "Epoch 45, Batch 10, Loss: 225943.0156\n",
      "Epoch 45/50, Loss: 7064.0130, LR: 3.00e-04\n",
      "Starting epoch 46/50\n",
      "Epoch 46, Batch 0, Loss: 225560.9844\n",
      "Epoch 46, Batch 10, Loss: 225574.0938\n",
      "Epoch 46/50, Loss: 7061.4228, LR: 3.00e-04\n",
      "Starting epoch 47/50\n",
      "Epoch 47, Batch 0, Loss: 225920.4375\n",
      "Epoch 47, Batch 10, Loss: 227150.7188\n",
      "Epoch 47/50, Loss: 7059.3678, LR: 3.00e-04\n",
      "Starting epoch 48/50\n",
      "Epoch 48, Batch 0, Loss: 226825.6719\n",
      "Epoch 48, Batch 10, Loss: 226943.0469\n",
      "Epoch 48/50, Loss: 7061.3186, LR: 3.00e-04\n",
      "Starting epoch 49/50\n",
      "Epoch 49, Batch 0, Loss: 226258.3906\n",
      "Epoch 49, Batch 10, Loss: 224972.3906\n",
      "Epoch 49/50, Loss: 7058.9446, LR: 3.00e-04\n",
      "Starting epoch 50/50\n",
      "Epoch 50, Batch 0, Loss: 224482.2500\n",
      "Epoch 50, Batch 10, Loss: 225934.4844\n",
      "Epoch 50/50, Loss: 7056.8134, LR: 3.00e-04\n",
      "Model saved to improved_vae_model.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "# Enhanced VAE with deeper architecture\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc_conv1 = nn.Conv2d(3, 32, 4, stride=2, padding=1)  # 64x64 -> 32x32\n",
    "        self.enc_conv2 = nn.Conv2d(32, 32, 3, stride=1, padding=1)\n",
    "        self.enc_bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.enc_conv3 = nn.Conv2d(32, 64, 4, stride=2, padding=1)  # 32x32 -> 16x16\n",
    "        self.enc_conv4 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
    "        self.enc_bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.enc_conv5 = nn.Conv2d(64, 128, 4, stride=2, padding=1)  # 16x16 -> 8x8\n",
    "        self.enc_conv6 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.enc_bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.enc_conv7 = nn.Conv2d(128, 256, 4, stride=2, padding=1)  # 8x8 -> 4x4\n",
    "        self.enc_conv8 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
    "        self.enc_bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Latent space\n",
    "        self.fc_mu = nn.Linear(256*4*4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256*4*4, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc_dec = nn.Linear(latent_dim, 256*4*4)\n",
    "        \n",
    "        self.dec_conv1 = nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1)  # 4x4 -> 8x8\n",
    "        self.dec_conv2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.dec_bn1 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.dec_conv3 = nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1)  # 8x8 -> 16x16\n",
    "        self.dec_conv4 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
    "        self.dec_bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.dec_conv5 = nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1)  # 16x16 -> 32x32\n",
    "        self.dec_conv6 = nn.Conv2d(32, 32, 3, stride=1, padding=1)\n",
    "        self.dec_bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.dec_conv7 = nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1)  # 32x32 -> 64x64\n",
    "        self.dec_conv8 = nn.Conv2d(16, 3, 3, stride=1, padding=1)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Encoder forward pass\n",
    "        h = F.leaky_relu(self.enc_conv1(x), 0.2)\n",
    "        h = F.leaky_relu(self.enc_bn1(self.enc_conv2(h)), 0.2)\n",
    "        h = F.leaky_relu(self.enc_conv3(h), 0.2)\n",
    "        h = F.leaky_relu(self.enc_bn2(self.enc_conv4(h)), 0.2)\n",
    "        h = F.leaky_relu(self.enc_conv5(h), 0.2)\n",
    "        h = F.leaky_relu(self.enc_bn3(self.enc_conv6(h)), 0.2)\n",
    "        h = F.leaky_relu(self.enc_conv7(h), 0.2)\n",
    "        h = F.leaky_relu(self.enc_bn4(self.enc_conv8(h)), 0.2)\n",
    "        h = h.view(h.size(0), -1)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        # Decoder forward pass\n",
    "        h = F.leaky_relu(self.fc_dec(z), 0.2)\n",
    "        h = h.view(-1, 256, 4, 4)\n",
    "        h = F.leaky_relu(self.dec_conv1(h), 0.2)\n",
    "        h = F.leaky_relu(self.dec_bn1(self.dec_conv2(h)), 0.2)\n",
    "        h = F.leaky_relu(self.dec_conv3(h), 0.2)\n",
    "        h = F.leaky_relu(self.dec_bn2(self.dec_conv4(h)), 0.2)\n",
    "        h = F.leaky_relu(self.dec_conv5(h), 0.2)\n",
    "        h = F.leaky_relu(self.dec_bn3(self.dec_conv6(h)), 0.2)\n",
    "        h = F.leaky_relu(self.dec_conv7(h), 0.2)\n",
    "        return torch.sigmoid(self.dec_conv8(h))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Dataset and DataLoader\n",
    "class VideoFrameDataset(Dataset):\n",
    "    def __init__(self, video_path, frame_size=(64, 64)):\n",
    "        self.frames = []\n",
    "        self.frame_size = frame_size\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(\"Cannot open video file: \" + video_path)\n",
    "        \n",
    "        print(f\"Loading video: {video_path}\")\n",
    "        frame_count = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_resized = cv2.resize(frame_rgb, self.frame_size)\n",
    "            frame_normalized = frame_resized.astype(np.float32) / 255.0\n",
    "            self.frames.append(frame_normalized)\n",
    "            frame_count += 1\n",
    "            if frame_count % 100 == 0:\n",
    "                print(f\"Loaded {frame_count} frames...\")\n",
    "        \n",
    "        cap.release()\n",
    "        print(f\"Finished loading. Total frames: {frame_count}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        frame = self.frames[idx]\n",
    "        return torch.from_numpy(frame).permute(2, 0, 1)\n",
    "\n",
    "# Loss function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Training loop\n",
    "def train_vae(vae, dataloader, num_epochs=50, learning_rate=1e-3, device='cuda'):\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "    \n",
    "    vae.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
    "        total_loss = 0\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = vae(batch)\n",
    "            loss = loss_function(recon_batch, batch, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        scheduler.step(avg_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, LR: {current_lr:.2e}')\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    vae = VAE(latent_dim=128).to(device)\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = VideoFrameDataset(\"input.mp4\", frame_size=(64, 64))\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    train_vae(vae, dataloader, num_epochs=50, learning_rate=3e-4, device=device)\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(vae.state_dict(), \"improved_vae_model.pth\")\n",
    "    print(\"Model saved to improved_vae_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a0875c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1080, 1920, 3)]   0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 540, 960, 32)      896       \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 540, 960, 32)      0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 270, 480, 64)      18496     \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 270, 480, 64)      0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 135, 240, 128)     73856     \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 135, 240, 128)     0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 270, 480, 64)     73792     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 270, 480, 64)      0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 540, 960, 32)     18464     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " re_lu_4 (ReLU)              (None, 540, 960, 32)      0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 1080, 1920, 3)    867       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 186,371\n",
      "Trainable params: 186,371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# 3. Train Autoencoder\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduce batch size if memory constrained\u001b[39;49;00m\n\u001b[0;32m     57\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# 4. Save Latent Representations\u001b[39;00m\n\u001b[0;32m     61\u001b[0m encoder \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39mautoencoder\u001b[38;5;241m.\u001b[39minput, outputs\u001b[38;5;241m=\u001b[39mautoencoder\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39moutput)\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\anish\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import subprocess\n",
    "import os\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "# 1. Data Preparation\n",
    "def load_and_preprocess_images(folder_path):\n",
    "    image_files = sorted(os.listdir(folder_path))\n",
    "    images = []\n",
    "    for file in image_files:\n",
    "        img = load_img(os.path.join(folder_path, file), target_size=(1080, 1920))\n",
    "        img_array = img_to_array(img) / 255.0\n",
    "        images.append(img_array)\n",
    "    return np.array(images)\n",
    "\n",
    "# Load all frames (be careful with memory)\n",
    "try:\n",
    "    frames = load_and_preprocess_images('./frames')\n",
    "except Exception as e:\n",
    "    print(f\"Error loading images: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Build Autoencoder\n",
    "def build_autoencoder():\n",
    "    input_img = Input(shape=(1080, 1920, 3))\n",
    "    \n",
    "    # Encoder\n",
    "    x = Conv2D(32, (3, 3), strides=2, padding='same')(input_img)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(64, (3, 3), strides=2, padding='same')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(128, (3, 3), strides=2, padding='same')(x)\n",
    "    encoded = ReLU()(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = Conv2DTranspose(64, (3, 3), strides=2, padding='same')(encoded)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2DTranspose(32, (3, 3), strides=2, padding='same')(x)\n",
    "    x = ReLU()(x)\n",
    "    decoded = Conv2DTranspose(3, (3, 3), strides=2, padding='same', activation='sigmoid')(x)\n",
    "    \n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder\n",
    "\n",
    "autoencoder = build_autoencoder()\n",
    "autoencoder.summary()\n",
    "\n",
    "# 3. Train Autoencoder\n",
    "autoencoder.fit(frames, frames,\n",
    "                epochs=50,\n",
    "                batch_size=2,  # Reduce batch size if memory constrained\n",
    "                shuffle=True,\n",
    "                validation_split=0.1)\n",
    "\n",
    "# 4. Save Latent Representations\n",
    "encoder = Model(inputs=autoencoder.input, outputs=autoencoder.layers[-4].output)\n",
    "os.makedirs('./latent_vectors', exist_ok=True)\n",
    "\n",
    "for i, img in enumerate(frames):\n",
    "    latent = encoder.predict(img[np.newaxis, ...])\n",
    "    np.save(f'./latent_vectors/frame_{i:04d}.npy', latent)\n",
    "\n",
    "# 5. Reconstruct Frames\n",
    "decoder_input = Input(shape=(135, 240, 128))\n",
    "decoder_layers = autoencoder.layers[-3](decoder_input)\n",
    "decoder_layers = autoencoder.layers[-2](decoder_layers)\n",
    "decoder_layers = autoencoder.layers[-1](decoder_layers)\n",
    "decoder = Model(decoder_input, decoder_layers)\n",
    "\n",
    "os.makedirs('./reconstructed_frames', exist_ok=True)\n",
    "\n",
    "for i in range(len(frames)):\n",
    "    latent = np.load(f'./latent_vectors/frame_{i:04d}.npy')\n",
    "    reconstructed = decoder.predict(latent)\n",
    "    reconstructed_img = (reconstructed[0] * 255).astype('uint8')\n",
    "    cv2.imwrite(f'./reconstructed_frames/frame_{i:04d}.png', cv2.cvtColor(reconstructed_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "# 6. Create Video\n",
    "subprocess.run([\n",
    "    'ffmpeg', '-framerate', '30', '-i', './reconstructed_frames/frame_%04d.png',\n",
    "    '-c:v', 'libx264', '-pix_fmt', 'yuv420p', '-vf', 'scale=1920:1080',\n",
    "    'output.mp4'\n",
    "])\n",
    "\n",
    "print(\"Processing complete! Output video saved as output.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016c6d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
